# ğŸ”§ PROMPT TEST TROUBLESHOOTING

## ğŸ¯ **Issue: Test Connection Works, Test Prompt Fails**

This is a common issue where:
- âœ… **Test Connection**: `/api/tags` endpoint works (can list models)
- âŒ **Test Prompt**: `/api/generate` endpoint fails (can't generate text)

## ğŸ” **Why This Happens**

1. **Tags vs Generate**: Different endpoints, different requirements
2. **Model Loading**: Model might be listed but not fully loaded
3. **Ollama State**: Ollama might be running but not ready for generation
4. **API Format**: Generate endpoint has stricter requirements

## ğŸš€ **Immediate Solutions**

### **Solution 1: Run Diagnostic**
Open browser console (F12) and run:
```javascript
diagnoseOllama()
```

This will tell you exactly what's wrong.

### **Solution 2: Manual Test**
In console run:
```javascript
testOllamaManual()
```

### **Solution 3: Fix Test Prompt Button**
In console run:
```javascript
fixPromptGeneration()
```

## ğŸ”§ **Common Fixes**

### **Fix 1: Restart Ollama**
```powershell
# Stop Ollama
Stop-Process -Name ollama -Force -ErrorAction SilentlyContinue

# Start Ollama
ollama serve
```

### **Fix 2: Load Model Properly**
```powershell
# Pull and load model
ollama pull llama3.2
ollama run llama3.2
# Wait for it to fully load, then exit
```

### **Fix 3: Check Ollama Status**
```powershell
# Check what's running
ollama list

# Check if serving
curl http://localhost:11434/api/tags
```

## ğŸ§ª **Manual Testing Steps**

### **Step 1: Test Tags Endpoint**
```javascript
fetch('http://localhost:11434/api/tags').then(r=>r.json()).then(console.log)
```
Should show your llama3.2 model.

### **Step 2: Test Generate Endpoint**
```javascript
fetch('http://localhost:11434/api/generate', {
    method: 'POST',
    headers: {'Content-Type': 'application/json'},
    body: JSON.stringify({
        model: 'llama3.2',
        prompt: 'Say "Hello"',
        stream: false
    })
}).then(r=>r.json()).then(console.log)
```

### **Step 3: Test Extension Button**
Click the "ğŸ§  Test Prompt" button again.

## ğŸ“Š **Expected Results**

### **âœ… Working Generate Endpoint:**
```json
{
  "model": "llama3.2",
  "created_at": "2024-01-01T00:00:00.000Z",
  "response": "Hello",
  "done": true,
  "total_duration": 123456789,
  "prompt_eval_count": 10,
  "eval_count": 5
}
```

### **âŒ Common Errors:**
- `model not found`: Model not loaded properly
- `connection refused`: Ollama not running
- `internal server error`: Ollama issue

## ğŸ¯ **Diagnostic Results Meaning**

### **All Green âœ…**: Everything working, try test prompt again
### **Tags âœ…, Generate âŒ**: Model issue, reload model
### **Both âŒ**: Ollama not running, restart Ollama

## ğŸ”„ **Complete Reset Procedure**

1. **Stop Everything**:
   ```powershell
   Stop-Process -Name ollama -Force -ErrorAction SilentlyContinue
   Stop-Process -Name msedge -Force -ErrorAction SilentlyContinue
   ```

2. **Start Ollama Fresh**:
   ```powershell
   ollama serve
   ```

3. **Load Model**:
   ```powershell
   ollama run llama3.2
   # Wait for complete load, then exit
   ```

4. **Start Extension**:
   - Open Edge with extension
   - Open console (F12)
   - Run `diagnoseOllama()`

5. **Test Again**:
   - Try "ğŸ”— Test Connection" 
   - Try "ğŸ§  Test Prompt"

## ğŸ‰ **Success Indicators**

- âœ… Diagnostic shows all endpoints working
- âœ… Test prompt button shows "Testing..." then success
- âœ… Console shows "Generate endpoint working!"
- âœ… Alert says "Prompt generation test successful!"

**ğŸš€ After these steps, the test prompt should work perfectly!**
